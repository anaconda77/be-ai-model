import os
import pickle
from datetime import datetime, timedelta

import finnhub
import numpy as np
import pandas as pd
import pandas_market_calendars as mcal
from dotenv import load_dotenv
from supabase import Client, create_client
from tensorflow.keras.models import load_model
from tqdm import tqdm

from source import (
    exceptions,
    input_processing,
    lstm_model,
    market_capitalization,
    sentiment_model,
)
from source.config import STOCK_LIST
from source.logging_config import setup_logging

logger = setup_logging()

SEQUENCE_LENGTH = 7
SENTIMENT_WINDOW_DAYS = 7
FETCH_DAYS = 13
pandas_ts = pd.Timestamp.now(tz="Asia/Seoul")


def get_price_data_from_db(supabase, stock_id):
    try:
        prices_latest_date_response = (
            supabase.table("stock_prices")
            .select("price_date")
            .eq("stock_id", stock_id)
            .order("price_date", desc=True)
            .limit(1)
            .single()
            .execute()
        )
    except Exception as e:
        # [ìˆ˜ì •] .single() ì—ëŸ¬ ë°œìƒ ì‹œ DataFetchErrorë¥¼ ë°œìƒì‹œì¼œ íŒŒì´í”„ë¼ì¸ ì¤‘ë‹¨
        raise exceptions.DataFetchError(
            f"'{stock_id}'ì— ëŒ€í•œ ìµœê·¼ ì£¼ê°€ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨ (DBì— ë°ì´í„°ê°€ ì—†ê±°ë‚˜ ì—ëŸ¬ ë°œìƒ): {e}"
        )

    prices_latest_date = prices_latest_date_response.data["price_date"]

    prices_response = (
        supabase.table("stock_prices")
        .select("*")
        .eq("stock_id", stock_id)
        .lte("price_date", prices_latest_date)
        .order("price_date", desc=True)
        .limit(FETCH_DAYS)
        .execute()
    )

    if not prices_response.data:
        raise exceptions.DataFetchError(
            f"'{stock_id}'ì— ëŒ€í•œ ì£¼ê°€ ìƒì„¸ ë°ì´í„°ê°€ DBì— ì—†ìŠµë‹ˆë‹¤."
        )

    return prices_response.data


def get_news_data_from_db(supabase, start_date, end_date, stock_id):

    news_response = (
        supabase.table("news")
        .select("*")
        .eq("stock_id", stock_id)
        .gte("published_date", start_date.strftime("%Y-%m-%d"))
        .lte("published_date", end_date.strftime("%Y-%m-%d"))
        .execute()
    )

    if not news_response.data:
        raise exceptions.DataFetchError(
            f"'{stock_id}'ì— ëŒ€í•œ ë‰´ìŠ¤ ë°ì´í„°ê°€ DBì— ì—†ìŠµë‹ˆë‹¤. (ê¸°ê°„: {start_date} ~ {end_date})"
        )

    return news_response.data


def get_existing_model(stock_code):
    """ëª¨ë¸ íŒŒì¼ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤. íŒŒì¼ì´ ì—†ìœ¼ë©´ ì—ëŸ¬ë¥¼ ë°œìƒì‹œí‚µë‹ˆë‹¤."""
    base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
    load_path = os.path.join(base_dir, "models", f"{stock_code}_finn_model.keras")
    if not os.path.exists(load_path):
        # [ìˆ˜ì •] FileNotFoundError ëŒ€ì‹  ë” êµ¬ì²´ì ì¸ ModelLoadError ì‚¬ìš©
        raise exceptions.ModelLoadError(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {load_path}")
    return load_model(load_path)


def get_existing_scaler(stock_code):
    """ìŠ¤ì¼€ì¼ëŸ¬ íŒŒì¼ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤. íŒŒì¼ì´ ì—†ìœ¼ë©´ ì—ëŸ¬ë¥¼ ë°œìƒì‹œí‚µë‹ˆë‹¤."""
    base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
    load_path = os.path.join(base_dir, "models", f"{stock_code}_finn_scaler.pkl")
    if not os.path.exists(load_path):
        # [ìˆ˜ì •] FileNotFoundError ëŒ€ì‹  ë” êµ¬ì²´ì ì¸ ModelLoadError ì‚¬ìš©
        raise exceptions.ModelLoadError(
            f"ìŠ¤ì¼€ì¼ëŸ¬ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {load_path}"
        )
    with open(load_path, "rb") as f:
        scaler = pickle.load(f)
    return scaler


def get_change_rate(prev_price, today_price):
    change_rate = ((today_price - prev_price) / prev_price) * 100
    return change_rate.round(2)


def get_closely_prev_close_price(df):
    # 2. 'close_price'ê°€ NaN(ë¹„ì–´ìˆì§€ ì•Šì€)ì´ ì•„ë‹Œ í–‰ë§Œ í•„í„°ë§í•©ë‹ˆë‹¤.
    valid_data_df = df.dropna(subset=["close_price"])

    # 3. ì¸ë±ìŠ¤(date)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬í•˜ì—¬ ê°€ì¥ ìµœì‹  ë°ì´í„°ê°€ ë§¨ ìœ„ë¡œ ì˜¤ê²Œ í•©ë‹ˆë‹¤.
    sorted_df = valid_data_df.sort_index(ascending=False)

    latest_valid_row = sorted_df.iloc[0]

    # ì¸ë±ìŠ¤ê°€ ë‚ ì§œì´ë¯€ë¡œ, .name ì†ì„±ìœ¼ë¡œ ë‚ ì§œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    latest_close_price = latest_valid_row["close_price"]

    return latest_close_price

def get_next_us_trading_day():
    """
    pandas-market-calendarsë¥¼ ì‚¬ìš©í•´ KST ì˜¤ëŠ˜ì„ ê¸°ì¤€ìœ¼ë¡œ
    ê°€ì¥ ê°€ê¹Œìš´ ë¯¸êµ­ ì¦ì‹œ(NYSE) ê°œì¥ì¼ì„ 'YYYY-MM-DD' í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    # NYSE ìº˜ë¦°ë”ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    nyse = mcal.get_calendar('NYSE')
    
    # í•œêµ­ ì‹œê°„ ê¸°ì¤€ ì˜¤ëŠ˜ ë‚ ì§œë¥¼ êµ¬í•©ë‹ˆë‹¤.
    today = datetime.now()
    
    # ì˜¤ëŠ˜ë¶€í„° í–¥í›„ 10ì¼ê°„ì˜ ê°œì¥ì¼ ìŠ¤ì¼€ì¤„ì„ ì¡°íšŒí•©ë‹ˆë‹¤.
    schedule = nyse.schedule(start_date=today.strftime('%Y-%m-%d'), 
                             end_date=(today + timedelta(days=10)).strftime('%Y-%m-%d'))
    
    # ì¡°íšŒëœ ìŠ¤ì¼€ì¤„ì˜ ì²« ë²ˆì§¸ ë‚ ì§œê°€ ë°”ë¡œ 'ê°€ì¥ ê°€ê¹Œìš´ ê°œì¥ì¼'ì…ë‹ˆë‹¤.
    if not schedule.empty:
        next_trading_day = schedule.index[0].date()
        return next_trading_day.strftime('%Y-%m-%d')
    else:
        # 10ì¼ ì•ˆì— ê°œì¥ì¼ì´ ì—†ëŠ” ê·¹ë‹¨ì ì¸ ê²½ìš°ì— ëŒ€í•œ ì˜ˆì™¸ ì²˜ë¦¬
        raise exceptions.TradingDayFoundError("í–¥í›„ 10ì¼ ë‚´ì— ê°œì¥ì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
# Predictions rowë¥¼ ë§Œë“¤ê³ , dbì— ì €ì¥(ì‹œê°€ì´ì•¡ ì •ë³´ë„ í˜¸ì¶œí•˜ì—¬ ì €ì¥)
def save_predictions_in_db(
    supabase,
    stock_id,
    stock_code,
    company_name,
    prediction_price,
    change_rate,
    capitalization,
):
    try:
        prediction_date = get_next_us_trading_day() # ê°€ì¥ ê°€ê¹Œìš´ ê°œì¥ì¼ ì°¾ì•„ì˜´
        response = (
            supabase.table("predictions")
            .upsert(
                {
                    "stock_id": stock_id,
                    "prediction_date": prediction_date,
                    "stock_code": stock_code,
                    "company_name": company_name,
                    "prediction_price": prediction_price,
                    "change_rate": change_rate,
                    "capitalization": capitalization,
                    "created_at": pandas_ts.strftime("%Y-%m-%dT%H:%M:%S%z"),
                }
            )
            .execute()
        )

        if hasattr(response, "error") and response.error is not None:
            logger.error(f"DB ì—…ë°ì´íŠ¸ ì¤‘ ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {response.error}")
        else:
            logger.info("âœ… DB ì—…ë°ì´íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        raise exceptions.DatabaseError(f"DB ì—…ë°ì´íŠ¸ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}") from e


def run_prediction_for_stock(supabase, finnhub_client, stock_code: str):
    """ë‹¨ì¼ ì£¼ì‹ ì½”ë“œì— ëŒ€í•œ ì „ì²´ ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    logger.info("=" * 50)
    logger.info(f"ğŸš€ {stock_code} ì˜ˆì¸¡ í”„ë¡œì„¸ìŠ¤ ì‹œì‘")
    logger.info("=" * 50)

    # 1. ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
    model = get_existing_model(stock_code)
    scaler = get_existing_scaler(stock_code)

    # 2. DBì—ì„œ ë°ì´í„° ì¡°íšŒ
    try:
        stock_response = (
            supabase.table("stocks")
            .select("id,stock_code,company_name")
            .eq("stock_code", stock_code)
            .single()
            .execute()
        )
    except Exception as e:
        # [ì¶”ê°€] .single() ì—ëŸ¬ ì²˜ë¦¬
        raise exceptions.DataFetchError(
            f"'{stock_code}' ì •ë³´ë¥¼ 'stocks' í…Œì´ë¸”ì—ì„œ ì¡°íšŒ ì‹¤íŒ¨: {e}"
        )

    if not stock_response.data:
        raise exceptions.DataFetchError(
            f"'{stock_code}' ì •ë³´ë¥¼ 'stocks' í…Œì´ë¸”ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        )

    stock_id = stock_response.data["id"]
    company_name = stock_response.data["company_name"]

    prices_data = get_price_data_from_db(supabase, stock_id)
    stock_prices_df = pd.DataFrame(prices_data)

    # 3. ë°ì´í„° ê²€ì¦ (ê¸¸ì´ í™•ì¸)
    if len(stock_prices_df) < FETCH_DAYS:
        raise exceptions.InsufficientDataError(
            f"ì£¼ê°€ ë°ì´í„° ë¶€ì¡±: ì˜ˆì¸¡ì— í•„ìš”í•œ {FETCH_DAYS}ì¼ì¹˜ ë°ì´í„° ì¤‘ {len(stock_prices_df)}ì¼ì¹˜ë§Œ ì¡°íšŒë¨."
        )

    # ... (ì´í›„ ë°ì´í„° ì²˜ë¦¬ ë¡œì§ì€ ê¸°ì¡´ê³¼ ë™ì¼) ...
    stock_prices_df = stock_prices_df.rename(
        columns={"price_date": "date", "id": "stock_price_id"}
    )
    start_date = pd.to_datetime(stock_prices_df["date"]).min()
    end_date = pd.Timestamp.now(tz='Asia/Seoul') + timedelta(days=1)

    news_response = get_news_data_from_db(supabase, start_date, end_date, stock_id)
    news_df = pd.DataFrame(news_response)
    news_df = news_df.rename(columns={"published_date": "date", "id": "news_id"})
    # ê°ì •í‰ê°€ ìˆ˜í–‰ í›„, sentiment_scoreë¥¼ db news í…Œì´ë¸”ì— ìƒˆë¡­ê²Œ ì—…ë°ì´íŠ¸í•œë‹¤.
    news_df = sentiment_model.analyze_sentiment_with_progress(news_df)

    merged_df = input_processing.merge_data(news_df, stock_prices_df)
    merged_df["sentiment_influence"] = 0.0
    merged_df = sentiment_model.add_integer_column(merged_df)
    sentiment_model.sums_sentiment_score_for_7_days(merged_df)
    sentiment_model.update_sentiment_score_in_db(supabase, merged_df)

    features = ["sentiment_influence", "open", "high", "low", "adjClose", "volume"]
    target = "close"
    all_cols = features + [target]
    dropped_merged_df = merged_df.rename(
        columns={
            "open_price": "open",
            "high_price": "high",
            "low_price": "low",
            "close_price": "close",
            "adj_close_price": "adjClose",
        }
    )
    dropped_merged_df = dropped_merged_df[features + [target]].dropna()

    # 4. ìµœì¢… ì…ë ¥ ë°ì´í„° ê²€ì¦
    if len(dropped_merged_df) < SEQUENCE_LENGTH:
        raise exceptions.InsufficientDataError(
            f"ìµœì¢… ë°ì´í„° ë¶€ì¡±: ëª¨ë¸ ì…ë ¥ì— í•„ìš”í•œ {SEQUENCE_LENGTH}ì¼ì¹˜ ë°ì´í„° ìƒì„± ì‹¤íŒ¨ ({len(dropped_merged_df)}ì¼ì¹˜ë§Œ ìƒì„±ë¨)."
        )

    # 5. ìŠ¤ì¼€ì¼ë§ ë° ì˜ˆì¸¡
    scaled = lstm_model.get_scale_data(scaler, dropped_merged_df)
    X = lstm_model.create_sequences_for_prod(scaled)

    y_pred_scaled = lstm_model.predict_prices(model, X)
    all_cols = [
        "sentiment_influence",
        "open",
        "high",
        "low",
        "adjClose",
        "volume",
        "close",
    ]
    features = all_cols[:-1]  # 'close'ë¥¼ ì œì™¸í•œ ëª¨ë“  ì»¬ëŸ¼
    target = "close"
    target_col_index = all_cols.index(target)

    dummy_array = np.zeros((len(y_pred_scaled), len(all_cols)))

    # 'close' ìœ„ì¹˜(6ë²ˆ ì¸ë±ìŠ¤)ì— ì˜ˆì¸¡ëœ ê°’ì„ ì‚½ì…
    dummy_array[:, target_col_index] = y_pred_scaled.ravel()
    # Scalerë¥¼ ì´ìš©í•´ ì „ì²´ ë°°ì—´ì„ ì—­ë³€í™˜í•˜ê³ , 'close' ì»¬ëŸ¼ë§Œ ì¶”ì¶œ
    y_pred_actual = scaler.inverse_transform(dummy_array)[:, target_col_index]

    next_day_predicted_close = y_pred_actual[-1].round(4)
    closely_prev_price = get_closely_prev_close_price(merged_df)
    change_rate = get_change_rate(closely_prev_price, next_day_predicted_close)
    logger.info(
        f"[{stock_code}] ëª¨ë¸ ì˜ˆì¸¡ ì™„ë£Œ. ì˜ˆì¸¡ ì¢…ê°€: ${next_day_predicted_close:.4f}"
    )

    logger.info(f"[{stock_code}] ì‹œê°€ì´ì•¡ ì •ë³´ ì¡°íšŒ ë° ìµœì¢… ê²°ê³¼ DB ì €ì¥ ì‹œì‘...")
    capitalization = market_capitalization.get_capitalization(
        finnhub_client, stock_code
    )
    save_predictions_in_db(
        supabase,
        stock_id,
        stock_code,
        company_name,
        next_day_predicted_close,
        change_rate,
        capitalization,
    )
    logger.info(f"[{stock_code}] ì˜ˆì¸¡ í”„ë¡œì„¸ìŠ¤ ì„±ê³µì ìœ¼ë¡œ ì¢…ë£Œ.")


def main():
    load_dotenv()

    try:
        SUPABASE_URL = os.environ.get("SUPABASE_URL")
        SUPABASE_KEY = os.environ.get("SUPABASE_KEY")
        FINNHUB_API_KEY = os.environ.get("FINNHUB_API_KEY")

        supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
        finnhub_client = finnhub.Client(api_key=FINNHUB_API_KEY)

        if not all([SUPABASE_URL, SUPABASE_KEY, FINNHUB_API_KEY]):
            raise exceptions.ConfigError(
                "í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜(SUPABASE_URL, SUPABASE_KEY, FINNHUB_API_KEY)ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            )

        """STOCK_LISTì— ìˆëŠ” ëª¨ë“  ì£¼ì‹ì— ëŒ€í•´ ì˜ˆì¸¡ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        logger.info("===== ì „ì²´ ì£¼ê°€ ì˜ˆì¸¡ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤ =====")

        for stock_code in STOCK_LIST:
            try:
                # ê° ì£¼ì‹ì— ëŒ€í•œ ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
                run_prediction_for_stock(supabase, finnhub_client, stock_code)
                tqdm.write(f"âœ… {stock_code} ì˜ˆì¸¡ ë° ì €ì¥ ì‘ì—… ì™„ë£Œ.")

            except (
                exceptions.ModelLoadError,
                exceptions.DataFetchError,
                exceptions.InsufficientDataError,
                exceptions.DatabaseError,
            ) as e:
                tqdm.write(
                    f"ğŸš¨ [{e.__class__.__name__}] {stock_code}: ì²˜ë¦¬ ê±´ë„ˆëœ€ (ì›ì¸: {e})"
                )

            except Exception as e:
                # â­ [ìˆ˜ì •] traceback.print_exc() ëŒ€ì‹  logger ì‚¬ìš©
                tqdm.write(
                    f"ğŸš¨ [ì‹¬ê°] {stock_code}: ì²˜ë¦¬ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì—ëŸ¬ ë°œìƒ. ë‹¤ìŒ ì¢…ëª©ìœ¼ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤. (ì›ì¸: {e})"
                )
                logger.error(f"[{stock_code}] ì˜ˆì¸¡ ë¶ˆê°€ ì˜¤ë¥˜ ë°œìƒ", exc_info=True)

    except Exception as e:
        logger.critical(
            f"ë©”ì¸ ì‘ì—… ì‹¤í–‰ ì¤‘ ì˜ˆì¸¡í•˜ì§€ ëª»í•œ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True
        )

    logger.info("\n===== ëª¨ë“  ì£¼ê°€ ì˜ˆì¸¡ ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤ =====")


if __name__ == "__main__":
    main()

import pandas as pd
import numpy as np
from tqdm.notebook import tqdm
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline

# â”€â”€â”€ ëª¨ë¸ ë¡œë”© (í•œ ë²ˆë§Œ ìˆ˜í–‰) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
model_name = "ProsusAI/finbert"
tokenizer  = AutoTokenizer.from_pretrained(model_name)
model      = AutoModelForSequenceClassification.from_pretrained(model_name)
# pipelineì— batch_sizeë¥¼ ì§€ì •í•´ ë†“ìœ¼ë©´, ë‚´ë¶€ì—ì„œ ìë™ ë°°ì¹˜ ì²˜ë¦¬ê°€ ë©ë‹ˆë‹¤.
finbert    = pipeline(
    "sentiment-analysis",
    model=model,
    tokenizer=tokenizer,
    device=0,         # GPUê°€ ìˆìœ¼ë©´ 0, ì—†ìœ¼ë©´ -1ë¡œ ì„¤ì •
    batch_size=16     # í•œ ë²ˆì— 16ê°œì”© ì²˜ë¦¬
)

def filtering_none_score_date(df):
    return df[df['sentiment_score'].isnull()].copy()   

def analyze_sentiment_with_progress_for_training(df: pd.DataFrame, batch_size: int = 32) -> pd.DataFrame:
    """
    dfì˜ 'title' ì»¬ëŸ¼ì„ batch_size ë‹¨ìœ„ë¡œ ë¬¶ì–´ì„œ finbert pipelineì— ë„˜ê¸°ê³ ,
    tqdmìœ¼ë¡œ ì§„í–‰ ìƒíƒœë¥¼ í‘œì‹œí•©ë‹ˆë‹¤. ë°˜í™˜ë˜ëŠ” DataFrameì€
    ['date','sentiment','confidence','sentiment_score'] ë„¤ ê°œ ì¹¼ëŸ¼ì„ ê°€ì§‘ë‹ˆë‹¤.
    """
    # 1) date, titleë§Œ ë‚¨ê¸´ ë³µì‚¬ë³¸ ë§Œë“¤ê¸°
    df2 = df[['date', 'title']].copy()
    n = len(df2)
    
    # 2) ê²°ê³¼ë¥¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸ ë¯¸ë¦¬ ìƒì„±
    all_labels = [None] * n
    all_scores = [None] * n
    
    # 3) ì´ ë°°ì¹˜ ê°œìˆ˜ ê³„ì‚°
    total_batches = int(np.ceil(n / batch_size))
    
    # 4) ì¸ë±ìŠ¤ë³„ë¡œ batch ì²˜ë¦¬í•˜ë©° tqdmìœ¼ë¡œ ì§„í–‰ ìƒí™© í‘œì‹œ
    for i in tqdm(range(total_batches), desc="ê°ì„± ë¶„ì„ ì§„í–‰ì¤‘", unit="batch"):
        start_idx = i * batch_size
        end_idx   = min(start_idx + batch_size, n)
        
        texts = df2['title'].iloc[start_idx:end_idx].astype(str).tolist()
        results = finbert(texts)  # ë¦¬ìŠ¤íŠ¸: [{ 'label': ..., 'score': ... }, ...]
        
        # ë°°ì¹˜ ê²°ê³¼ë¥¼ ë¯¸ë¦¬ ë§Œë“  ë¦¬ìŠ¤íŠ¸ì— ì €ì¥
        for j, res in enumerate(results, start=start_idx):
            all_labels[j] = res['label']
            all_scores[j] = round(res['score'], 4)
    
    # 5) ë¦¬ìŠ¤íŠ¸ë¥¼ ì¹¼ëŸ¼ì— í• ë‹¹
    df2['sentiment']  = all_labels
    df2['confidence'] = all_scores
    
    return df2
    
# â”€â”€â”€ ë°°ì¹˜+ì§„í–‰ ë°” ë²„ì „ ê°ì„± ë¶„ì„ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def analyze_sentiment_with_progress(df: pd.DataFrame, batch_size: int = 32) -> pd.DataFrame:
    """
    dfì˜ 'title' ì»¬ëŸ¼ì„ batch_size ë‹¨ìœ„ë¡œ ë¬¶ì–´ì„œ finbert pipelineì— ë„˜ê¸°ê³ ,
    tqdmìœ¼ë¡œ ì§„í–‰ ìƒíƒœë¥¼ í‘œì‹œí•©ë‹ˆë‹¤. ë°˜í™˜ë˜ëŠ” DataFrameì€
    ['date','sentiment','confidence','sentiment_score'] ë„¤ ê°œ ì¹¼ëŸ¼ì„ ê°€ì§‘ë‹ˆë‹¤.
    """
    #filtering_df = filtering_none_score_date(df)
    
    # 1) date, titleë§Œ ë‚¨ê¸´ ë³µì‚¬ë³¸ ë§Œë“¤ê¸°
    df2 = df[['news_id', 'date', 'title']].copy()
    n = len(df2)
    
    # 2) ê²°ê³¼ë¥¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸ ë¯¸ë¦¬ ìƒì„±
    all_labels = [None] * n
    all_scores = [None] * n
    
    # 3) ì´ ë°°ì¹˜ ê°œìˆ˜ ê³„ì‚°
    total_batches = int(np.ceil(n / batch_size))
    
    # 4) ì¸ë±ìŠ¤ë³„ë¡œ batch ì²˜ë¦¬í•˜ë©° tqdmìœ¼ë¡œ ì§„í–‰ ìƒí™© í‘œì‹œ
    for i in tqdm(range(total_batches), desc="ê°ì„± ë¶„ì„ ì§„í–‰ì¤‘", unit="batch"):
        start_idx = i * batch_size
        end_idx   = min(start_idx + batch_size, n)
        
        texts = df2['title'].iloc[start_idx:end_idx].astype(str).tolist()
        results = finbert(texts)  # ë¦¬ìŠ¤íŠ¸: [{ 'label': ..., 'score': ... }, ...]
        
        # ë°°ì¹˜ ê²°ê³¼ë¥¼ ë¯¸ë¦¬ ë§Œë“  ë¦¬ìŠ¤íŠ¸ì— ì €ì¥
        for j, res in enumerate(results, start=start_idx):
            all_labels[j] = res['label']
            all_scores[j] = round(res['score'], 4)
    
    # 5) ë¦¬ìŠ¤íŠ¸ë¥¼ ì¹¼ëŸ¼ì— í• ë‹¹
    df2['sentiment']  = all_labels
    df2['confidence'] = all_scores
    
    return df2

def add_integer_column(df):
    # 6) ì •ìˆ˜í˜• ì ìˆ˜ ì»¬ëŸ¼ ì¶”ê°€
    sentiment_map = {'positive': 1, 'neutral': 0, 'negative': -1}
    df['sentiment_score'] = df['sentiment'].map(sentiment_map)
    
    # 7) date ì¸ë±ìŠ¤ë¡œ ì„¤ì •í•˜ê±°ë‚˜, ì›í•œë‹¤ë©´ ë¦¬í„´ ì „ì— reset_index() í•´ë„ ë©ë‹ˆë‹¤
    df.set_index('date', inplace=True)
    return df
    
def sums_sentiment_score_for_7_days(df):
    weights = np.linspace(1.0, 0.1, 7)  # 1.0 â†’ 0.1ê¹Œì§€ 7ë‹¨ê³„ë¡œ ì¤„ì–´ë“¦
    df['sentiment_influence'] = 0.0
    
    for date, row in tqdm(df.iterrows(), total=len(df)):
        # neutral ë˜ëŠ” NaNì€ ê±´ë„ˆëœ€
        if pd.isna(row['sentiment_score']) or pd.isna(row['confidence']):
            continue

        base = row['sentiment_score'] * row['confidence']

        # 7ì¼ ë™ì•ˆ ê°€ì¤‘ì¹˜ ì ìš©í•´ì„œ ëˆ„ì 
        for i in range(7):
            apply_date = date + pd.Timedelta(days=i)
            if apply_date in df.index:
                df.at[apply_date, 'sentiment_influence'] += base * weights[i]
                

def update_sentiment_score_in_db(supabase_client, df_to_update):
    if df_to_update.empty:
        print("â„¹ï¸ DBì— ì—…ë°ì´íŠ¸í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    if 'news_id' not in df_to_update.columns or 'sentiment_score' not in df_to_update.columns:
        print("ğŸš¨ ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•´ DataFrameì— 'news_id'ì™€ 'sentiment_score' ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        return

    print(f"ğŸ”„ {len(df_to_update)}ê°œì˜ ê°ì„± ì ìˆ˜ë¥¼ DBì— ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤...")
    
    records_to_update = df_to_update[['news_id', 'sentiment_score']].dropna()
    
    if records_to_update.empty:
        print("â„¹ï¸ ìœ íš¨í•œ ì—…ë°ì´íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 2. (í•µì‹¬ ë³€ê²½ì‚¬í•­) ê° í–‰ì„ '(id, score)' í˜•íƒœì˜ 'ë¬¸ìì—´'ë¡œ ë³€í™˜
    update_strings = []
    for index, row in records_to_update.iterrows():
        # PostgreSQL ë ˆì½”ë“œ ë¦¬í„°ëŸ´ í˜•ì‹: '(ê°’1, ê°’2)'
        # UUIDëŠ” ë¬¸ìì—´ì´ë¯€ë¡œ ì‘ì€ ë”°ì˜´í‘œ('')ë¡œ ê°ì‹¸ì¤ë‹ˆë‹¤.
        record_literal = f"({row['news_id']}, {row['sentiment_score']})"
        update_strings.append(record_literal)

    try:
        # 3. Supabaseì˜ rpc ê¸°ëŠ¥ì„ ì‚¬ìš©í•´ DB í•¨ìˆ˜ í˜¸ì¶œ
        # íŒŒë¼ë¯¸í„°ë¡œ 'ë¬¸ìì—´ì˜ ë¦¬ìŠ¤íŠ¸'ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤.
        response = supabase_client.rpc(
            'update_batch_sentiment_scores',
            {'updates': update_strings}
        ).execute()

        # ... (ì´í›„ ì—ëŸ¬ ì²˜ë¦¬ ë° ì„±ê³µ ë©”ì‹œì§€ ì¶œë ¥ ë¡œì§ì€ ì´ì „ê³¼ ë™ì¼) ...
        if hasattr(response, 'error') and response.error is not None:
            print(f"ğŸš¨ DB ì—…ë°ì´íŠ¸ ì¤‘ ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {response.error}")
        else:
            print("âœ… DB ì—…ë°ì´íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"ğŸš¨ DB ì—…ë°ì´íŠ¸ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
    